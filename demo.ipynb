{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to a typical Kafka administrator nightmare..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a JSONSchema-serialized topic \"products\" and a qualification test is looming for an application consuming from that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of course, the application crashes...\n",
    "### (run consumer.py)\n",
    "### Error: confluent_kafka.schema_registry.error.SchemaRegistryError: Schema 577659245 not found (HTTP status code 404, SR code 40403)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The application cannot be fixed as it has been developed by externals (sigh), so we, the Kafka administrators, have to jump in and save the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we have to find out what is wrong with the topic.\n",
    "### The error message indicates that there are messages which have not been JSONSchema-serialized but are pure JSON.\n",
    "### Let's validate that assumption and read the first message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafi.kafi import *\n",
    "c = Cluster(\"local\")\n",
    "x = c.head(\"products\", type=\"bytes\", n=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, so how many messages are pure JSON instead of JSONSchema-serialized (value does not start with the magic zero byte)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = c.filter(\"products\", type=\"bytes\", filter_function=lambda x: x[\"value\"][0] != 0)\n",
    "print(len(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, it's the first 100. Let's do a backup of them to a topic backed by local Kafi Kafka emulation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Local(\"local\")\n",
    "l.retouch(\"products_backup\")\n",
    "c.cp(\"products\", l, \"products_backup\", source_type=\"json\", target_type=\"json\", offsets={0: 0}, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And then delete the first 100 messages on the real Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.delete_records({\"products\": {0: 100}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the watermarks whether this has really worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.watermarks(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, run the consumer application again...\n",
    "### (run consumer.py)\n",
    "### Works now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we do have to bring back the first 100 messages, because the producers cannot do that (again, externals...).\n",
    "### Let's see what schema ID we have to use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = c.head(\"products\", type=\"bytes\", n=1)\n",
    "sid = int.from_bytes(z[0][\"value\"][1:5], \"big\")\n",
    "print(sid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, let's use that schema ID for adding the backed up messages to the end of the products topic - correctly JSONSchema-serialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.cp(\"products_backup\", c, \"products\", target_value_type=\"jsonschema\", target_value_schema_id=sid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argh, does not work: SerializationError: 'price' is a required property\n",
    "### What's up?\n",
    "### Let's check in Excel..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.to_file(\"products_backup\", l, \"products_backup.xlsx\", n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argh, they have misspelled \"price\" as \"pryce\" (you might know the movie \"Brazil\" with Jonathan Pryce, it's similarly dystopian as our task at hand...)\n",
    "### So let's fix that typo in-place in the Kafi Kafka-emulated topic on disk...\n",
    "### (fix topic in VSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And try to copy it to the back of the \"products\" topic again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.cp(\"products_backup\", c, \"products\", target_value_type=\"jsonschema\", target_value_schema_id=sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.watermarks(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yeah.\n",
    "### Now let's check if the consumer application can read all the messages, including the fixed ones...\n",
    "### (run consumer.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We did it :) Now the qualification test can go through. Phew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very last step: Let's create a copy of that fixed topic in Parquet format for the analytic team - on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = S3(\"local\")\n",
    "c.to_file(\"products\", s, \"products.parquet\", type=\"jsonschema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see...\n",
    "### (download Parquet file, show it, open a URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's really it.\n",
    "### Thanks to all my colleagus from Migros in ZÃ¼rich, in particular the Data Integration team, and Martin Muggli and Jason Nguyen - the Kafka guys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your copy of Kafi from GitHub: https://github.com/xdgrulez/kafi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And get your copy of the new O'Reilly book \"Streaming Databases\" by Hubert Dulay and me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And hand in exciting abstracts to the new, non-vendor-centric conference about everything events and streaming: EventCentric 2025 (Antwerp, Belgium, June 2-5, 2025)\n",
    "### https://aardling.eu/en/eventcentric-2025-coming-soon\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
